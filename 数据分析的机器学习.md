## 一、机器学习基础知识

### 1、特征工程

#### 1.1 为什么要对特征做归一化处理？

消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化。

#### 1.2 什么是组合特征？如何处理高维组合特征？

为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组 合，构成高阶组合特征。可以采取降维，矩阵分解或者特征筛选的方法得到具备绝大部分信息的几个特征进行训练。

#### 1.3 如何计算样本之间的相似度？-距离度量

**计算样本之间的距离**，无论是监督学习还是无监督学习，都会使用距离度量。这些度量，如欧几里得距离或者余弦相似性，经常在 k-NN、 UMAP、HDBSCAN 等算法中使用。

**9中距离度量方法**

![img](https://pic2.zhimg.com/80/v2-7946eeeeb36ab470d6415e7a0eaa10f9_720w.webp)

##### **（1）欧氏距离（Euclidean Distance）**

- 概念

欧几里得距离是连接两个点的线段的长度。欧氏距离越小，两个向量的相似度越大；欧氏距离越大，两个向量的相似度越小。**（直线距离）**

- 计算公式：

![img](https://pic2.zhimg.com/80/v2-a3646ffe6bb4b90b3d6f0efe6e1329b5_720w.webp)

- 特点
  - 值受特征的单位的影响。通常，在使用欧式距离度量之前，需要对数据进行归一化处理。
  - 随着数据维数的增加，欧氏距离的作用也就越小。这与维数灾难（curse of dimensionality）有关。
  - 在低维数据且向量的大小非常重要时，欧式距离的效果非常好。如果在低维数据上使用欧式距离，则如 k-NN 和 HDBSCAN 之类的方法可达到开箱即用的效果。

##### **（2）余弦相似度（Cosine Similarity）**

-   概念

余弦相似度是指两个向量夹角的余弦。两个方向完全相同的向量的余弦相似度为 1，而两个彼此相对的向量的余弦相似度为 - 1。注意，它们的大小并不重要，因为这是在方向上的度量。

- 计算公式：

![img](https://pic3.zhimg.com/80/v2-fbd0ce9eecb1836d40e8050619c79632_720w.webp)

- 特点
  - 没有考虑向量的大小，而只考虑它们的方向。以推荐系统为例，余弦相似度就没有考虑到不同用户之间**评分尺度**的差异。
  - 对**高维数据向量**的大小不关注时，可以使用余弦相似度。

##### **（3）曼哈顿距离**（Manhattan Distance）****

曼哈顿距离通常称为出租车距离或城市街区距离，用来计算实值向量之间的距离。|x1 - x2| + |y1 - y2|

![img](https://pic4.zhimg.com/80/v2-a40092e31afe3635c00c0ea3e587e7c3_720w.webp)

- 高维数据中似乎可以工作，但它比欧式距离直观性差，且由于它可能不是最短路径，有可能比欧氏距离给出一个更高的距离值。
- 当数据集具有离散或二进制属性时，曼哈顿距离似乎工作得很好，因为它考虑了在这些属性的值中实际可以采用的路径。

##### **（4）切比雪夫距离**（Chebyshev Distance）

定义为两个向量在任意坐标维度上的最大差值。换句话说，它就是沿着一个轴的最大距离。切比雪夫距离通常被称为棋盘距离，因为国际象棋的国王从一个方格到另一个方格的最小步数等于切比雪夫距离。max（|x1 - x2|，|y1 - y2|）

**适用场景**：切比雪夫距离可用于提取从一个方块移动到另一个方块所需的最小移动次数。此外，在允许无限制八向移动的游戏中，这可能是有用的方法。在实践中，切比雪夫距离经常用于**仓库物流**，因为它非常类似于起重机移动一个物体的时间。

#### 1.5 one-hot编码

one-hot 编码用于将离散的分类标签转换为二进制向量。

![img](https://pic2.zhimg.com/80/v2-19752f969172da3d75e0ae9e484c4309_720w.webp)

one-hot编码怎么用？
配合softmax ，softmax 激活函数输出一系列概率值，加起来为1.左侧是softmax的输出，两者计算损失函数。

局限性：维度灾难。如果分类数量有1万个，我们是不是需要将1万个离散的分类，编码成1万维的向量来计算呢？

![img](https://pic3.zhimg.com/80/v2-eba5ea62115bf4fb2618f2cd0d55c266_720w.webp)

### 2、模型评估

####  2.1 过拟合和欠拟合

##### （1）两者的区别

过拟合：训练集上表现很好，测试集上表现很差

欠拟合：训练集上表现差

##### （2）解决欠拟合的方法

- 1、**加特征**：增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间;
- 2、**改模型**: 尝试非线性模型，比如核SVM 、决策树、DNN等模型;
- 3、如果有正则项可以较小正则项参数 $\lambda$;
- 4、**集成学习**（增加训练次数）：Boosting ,Boosting 往往会有较小的 Bias，比如 Gradient Boosting 等

##### （3）解决过拟合的方法

![img](https://mmbiz.qpic.cn/mmbiz_png/Xo9n2nzuNDWqAW0Jva5veSS10ZuYhdYiahNBw9xHnQfSEq2voHZfrRpUnxIgicTgoYgc2BU5G6BBViazwria2YGjMA/640?wx_fmt=png)

####  **2.2. 评价指标**

（1）误差

Error = Bias + Variance + Noise

Error反映的是整个模型的准确度。Bias反映的是模型在样本上的输出与真实值之间的误差，即模型**本身的精准度**，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即**模型的稳定性**。

### 3、集成学习

#### 3.1 什么是集成学习算法？

集成学习算法是通过构建并结合多个机器学习器来完成学习任务。可以说是集百家之所长，能在机器学习算法中拥有较高的准确率，不足之处就是模型的训练过程可能比较复杂，效率不是很高。

目前常见的集成学习算法主要有2种：基于Bagging的算法和基于Boosting的算法，基于Bagging的代表算法有随机森林，而基于Boosting的代表算法则有Adaboost、GBDT、XGBOOST等。

![img](https://pic1.zhimg.com/80/v2-9c8976f361643d26f5304b3664e80bc4_720w.webp)

#### 3.2 集成学习Bagging，Boosting、Stacking

**（1）Boosting**

基分类器层层叠加，聚焦分错的样本，旨在减小方差。主要思想：迭代式学习。AdaBoost，GBDT，XGBoost都属于Boosting思想。

流程：1、给定初始训练数据，由此训练出第一个基学习器；2、根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；3、用调整后的样本，训练下一个基学习器；4、重复上述过程T次，将T个学习器加权结合

![img](https://mmbiz.qpic.cn/mmbiz_png/Xo9n2nzuNDWqAW0Jva5veSS10ZuYhdYia5WNxhTSX3iceLy1r0fdGqb3ZbedSqKOJYs4Fn8jJtAQ9oJeJEvGcEwA/640?wx_fmt=png)

**（2）Bagging**

多次采样（bootstrap （有放回的随机抽样）每次从训练集中抽取一个固定大小的训练集A,**随机重抽样**），训练多个分类器，**集体投票**，旨在减小方差。bagging算法：随机森林算法

![img](https://mmbiz.qpic.cn/mmbiz_png/Xo9n2nzuNDWqAW0Jva5veSS10ZuYhdYiabtUsLWZHPgEjLGulpCtBYyb6xa5QPdS4E5GPRYd53usOMicODMVDkCA/640?wx_fmt=png)

**（3）stacking**

多次采样，训练多个分类器，将输出作为最后的输入特征,由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的Loqistic 回归组合。**多个特征组合**

![img](https://mmbiz.qpic.cn/mmbiz_png/Xo9n2nzuNDWqAW0Jva5veSS10ZuYhdYia3EhzSdaicDz6wNm3YPlrCHicGAMPUHpyibObe1SBejgVLwtbIKjeQLlDQ/640?wx_fmt=png)

## 二、机器学习相关算法

### 1、决策树

#### 1.1决策树是什么

 决策树是一种基于**二叉树**（最多有左右两个子树）的机器学习模型。

决策树遍历训练数据并将信息浓缩为二叉树的内部节点和叶节点，从而学习训练集中的观测值之间的关系，这些观测值表示为特征向量x和目标值y。

决策树中的每个叶子都表示特定的预测结果。回归树中输出的预测是一个（连续的）值，例如价格；而分类树中输出的预测是（离散的）目标类别（在scikit中表示为整数）。**对于回归而言，叶节点中观测的相似性意味着目标值之间的差异很小；而对于分类而言，则意味着大多数或所有观测属于同一类别。**

#### 1.2 决策树构建

就是不停的if_else, 在特征空间里选择合适的决策分割点不断对样本进行二分类，当达到某个停止标准（例如，节点中包含的观测数少于5）时，决策树终止生长。简而言之，决策树的建立就是在自变量x中寻找最合适的切割值，将样本分成亚组，从而可以将因变量进行更加**同质化的归类。**

**构建决策树的关键在于如何选择特征划分（分割点的选择，节点的设置）**

这些标准都是想最大程度上保证分类后样本具有一致性，这样分类才是更加有效的

**衡量标准：**

**（1）信息熵**

![1702468231809](C:\Users\hp\AppData\Roaming\Typora\typora-user-images\1702468231809.png)

![1702468294236](C:\Users\hp\AppData\Roaming\Typora\typora-user-images\1702468294236.png)

**（2）信息增益**

**（3）信息增益率**

当遇到类似于ID的稀疏特征时，以该稀疏特征作为节点，实现了完美分类，但显然该稀疏特征如果和目标特征没啥关系，新样本用该决策树进行预估类别时就是错误。

面对这样的情况，增添了信息增益率作为节点分割的衡量标准

**（4）GINI系数**



![img](https://mmbiz.qpic.cn/mmbiz_png/Xo9n2nzuNDWqAW0Jva5veSS10ZuYhdYiauuOIULJNwUd28zoLV9KLyCR80447px3eBbQdicPibdluUUmR1S9eLicRQ/640?wx_fmt=png)

### 2、随机森林

#### 2.1概念

随机：数据采样随机，特征选择随机

森林：多个决策树独立，对预测结果加权

#### **2.2随机森林训练模型时需要考虑的超参数：**

（1）树的数量：最好特征数量的10呗

（2）特征分割时需要考虑的特征数量：Metry：对于回归问题，默认值为p/3

（3）树的复杂度：节点大小，最大深度，终端节点最大数量等

（4）抽样方案: bootstrap,100%重复抽样，是否替换抽样，单个树的样本量大小，减少样本量可以帮助增加树的多样性，降低树之间的相关性，从而对预测精度产生积极影响

（5）树构建过程中所使用的拆分规则

#### **2.3为什么随机森林可以输出特征重要性** 

（1）方法一：基于袋外数据，置换袋外数据中特征Xi，OBB误差率的差值就是Xi特征重要性评分

袋外数据就是在建立某棵决策树时不被选择的样本集，可以作为该决策树的测试数据集

但对于整个森林而言，袋外数据仍是训练数据

（2）方法二：基于GINI系数，特征Xi的每一个节点的GINI系数差综合（GINI系数差=未分裂节点的GINI系数 - 分裂后分出的左右节点GINI系数和）

#### 2.4shap解释RF

```python
import shap
from sklearn.externals import joblib
from sklearn.ensemble import RandomForestRegressor

#导入训练好的模型
model = RandomForestRegressor(n_estimators = n_com, max_depth = None,
                               random_state = 0, bootstrap = True,
                               oob_score = True).fit(X,y)

#model = joblib.load('..（你的随机森林模型路径）\RF.model')
#创建解释器
explainer = shap.TreeExplainer(model) 

#用解释器对特征参数进行解释
#x_test为特征参数数组 shap_value为解释器计算的shap值
shap_values = explainer.shap_values(x_test)

#这里解释的都是全局可解释性
#重要性排序绘图
shap.summary_plot(shap_values, x_test, plot_type="bar",show=False)
#重要性排序图（带正负影响）
shap.summary_plot(shap_values, x_test)
```



### 3、GBDT



### 4、XGBoost

### 5、Adaboost 

### 5、K-means

### 6、支持向量机（SVM）



## 三、深度学习基础知识

### 1、激活函数

### 2、梯度消失和爆炸

### 3、Softmax

### 4、BN/LN/IN/GN的区别

## 四、深度学习相关算法

### 1、循环神经网络

### 2、注意力机制

### 3、Transformer

### 4、Bert

